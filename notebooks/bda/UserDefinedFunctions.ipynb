{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGxKyRjWkmba"
      },
      "source": [
        "# User-defined functions\n",
        "\n",
        "Si bien Apache Spark cuenta con una gran cantidad de funciones incorporadas, la flexibilidad de Spark permite que los ingenieros de datos y los científicos de datos también definan sus propias funciones. Estas son conocidas como funciones definidas por el usuario (UDFs).\n",
        "\n",
        "\n",
        "El beneficio de crear tus propias UDFs (funciones definidas por el usuario) en PySpark o Scala la reusabilidad de dichas funciones para todos los componentes del equipo de desarrollo. Por ejemplo, un científico de datos puede introducir un modelo de aprendizaje automático (ML) dentro de una UDF para que un analista de datos pueda consultar sus predicciones en Spark sin necesidad de entender los detalles internos del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZOo1smyud_Q",
        "outputId": "f6f755e5-bee5-40b3-a2ea-2388c128397a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdXiZ2_UuXQU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql2W5tvHlh1y"
      },
      "source": [
        "## Primer ejemplo básico -> Elevar al cubo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am-9hc3zlbgx",
        "outputId": "ac95e4ad-3d9f-4582-add0-b30ea6055fb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.cube(s)>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Función para elevar al cubo\n",
        "def cube(s):\n",
        "  return s * s * s\n",
        "\n",
        "# Registro de la UDF\n",
        "spark.udf.register('cube', cube, LongType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QfmQxZvl9On"
      },
      "outputs": [],
      "source": [
        "spark.range(1, 10).createOrReplaceTempView('udf_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIzVwbdSmH-E",
        "outputId": "d2d636c4-fe55-4e6c-dc07-a5639f5900bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('select * from udf_test').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIhdg8LfmR1e"
      },
      "outputs": [],
      "source": [
        "df = spark.sql('select id, cube(id) as cube from udf_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6JMJYvtqUp_",
        "outputId": "f452c76e-9e3c-453c-9b44-62075b730819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|cube|\n",
            "+---+----+\n",
            "|  1|   1|\n",
            "|  2|   8|\n",
            "|  3|  27|\n",
            "|  4|  64|\n",
            "|  5| 125|\n",
            "|  6| 216|\n",
            "|  7| 343|\n",
            "|  8| 512|\n",
            "|  9| 729|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qS8O_Zjmo7D"
      },
      "source": [
        "## Aceleración de UDFs con UDFs de Pandas\n",
        "\n",
        "Uno de los problemas al utilizar UDFs de PySpark era que tenían un rendimiento más lento que las UDFs de Scala. Esto se debía a que las UDFs de PySpark requerían el movimiento de datos entre el JVM y Python, lo cual era bastante costoso. Para resolver este problema, se introdujeron las UDFs de Pandas (también conocidas como UDFs vectorizadas) como parte de Apache Spark 2.3. Una UDF de Pandas utiliza Apache Arrow para transferir datos y Pandas para trabajar con los datos.\n",
        "\n",
        "Una UDF de Pandas se define utilizando la palabra clave pandas_udf como decorador. En lugar de operar en entradas individuales fila por fila, se opera en una Serie de Pandas (realiza una ejecución vectorizada)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9To4SjGqmYzG",
        "outputId": "5e873ae6-763e-41c4-a62a-7de260f965db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|add_one(id)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "|          5|\n",
            "|          6|\n",
            "|          7|\n",
            "|          8|\n",
            "|          9|\n",
            "|         10|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "@pandas_udf('integer')\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "  return s + 1\n",
        "\n",
        "spark.udf.register(\"add_one\", add_one)\n",
        "spark.sql(\"SELECT add_one(id) FROM udf_test\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOYnISfqAGj"
      },
      "source": [
        "## Otro ejemplo -> con varias columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf6znZNYmwZa",
        "outputId": "6370a041-e107-4c3e-a7b4-e972663756d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+------------+\n",
            "| id|cube|ratio_column|\n",
            "+---+----+------------+\n",
            "|  1|   1|         1.0|\n",
            "|  2|   8|        0.25|\n",
            "|  3|  27|        0.11|\n",
            "|  4|  64|        0.06|\n",
            "|  5| 125|        0.04|\n",
            "|  6| 216|        0.03|\n",
            "|  7| 343|        0.02|\n",
            "|  8| 512|        0.02|\n",
            "|  9| 729|        0.01|\n",
            "+---+----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import pandas_udf, round\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "@pandas_udf(DoubleType())\n",
        "def calculate_ratio(column1, column2):\n",
        "    return column1 / column2\n",
        "\n",
        "df.withColumn('ratio_column', round(calculate_ratio(df['id'], df['cube']), 2)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJvkPlu7rFHb"
      },
      "source": [
        "## Otro ejemplo con una función personalizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVXHPvhFrEJm",
        "outputId": "e6c23ea3-48a4-4809-ba15-cc0febef83a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+----------------+\n",
            "| id|cube|processed_column|\n",
            "+---+----+----------------+\n",
            "|  1|   1|             {1}|\n",
            "|  2|   8|             {0}|\n",
            "|  3|  27|             {9}|\n",
            "|  4|  64|             {0}|\n",
            "|  5| 125|            {25}|\n",
            "|  6| 216|             {0}|\n",
            "|  7| 343|            {49}|\n",
            "|  8| 512|             {0}|\n",
            "|  9| 729|            {81}|\n",
            "+---+----+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "def my_func(x):\n",
        "  if x % 2 == 0:\n",
        "    return 0\n",
        "  return x * x\n",
        "\n",
        "@pandas_udf(StructType([StructField('result', IntegerType())]))\n",
        "def process_text(series):\n",
        "    processed_data = series.apply(lambda x: my_func(x))\n",
        "    return pd.DataFrame({'result': processed_data})\n",
        "\n",
        "df.withColumn('processed_column', process_text(df['id'])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly6NhihPs9R_"
      },
      "source": [
        "Para ampliar conocimientos acerca de UDFs podéis acceder al siguiente enlace de documentación de Databricks:\n",
        "\n",
        "https://docs.databricks.com/en/udf/pandas.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
