{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAuMfxKk9ABE"
      },
      "source": [
        "# Tipos Complejos\n",
        "\n",
        "Explora las funciones integradas para trabajar con colecciones y cadenas.\n",
        "\n",
        "##### Objetivos\n",
        "1. Aplicar funciones de colección para procesar arrays\n",
        "2. Unir DataFrames\n",
        "\n",
        "##### Métodos\n",
        "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`union`**, **`unionByName`**\n",
        "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\" target=\"_blank\">Funciones Integradas</a>:\n",
        "  - Agregado: **`collect_set`**\n",
        "  - Colección: **`array_contains`**, **`element_at`**, **`explode`**\n",
        "  - Cadena: **`split`**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afMHglFxwCr5",
        "outputId": "e0e9d32c-cd70-49b0-9922-38d5e846a3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=dd2dd60864c03876316f7a3ecdf3085f10ad493498e3b732e1b021131b16fd3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep6PJgNzwFQB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('complex-types').getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNbTc8YaB_v1",
        "outputId": "cb81ef66-e86e-4fb3-c7d0-3fddfd70dc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "| id|  valores|\n",
            "+---+---------+\n",
            "|  A|[1, 2, 3]|\n",
            "|  B|   [4, 5]|\n",
            "|  C|      [6]|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"A\", [1, 2, 3]),\n",
        "        (\"B\", [4, 5]),\n",
        "        (\"C\", [6])]\n",
        "\n",
        "columns = ['id', 'valores']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNZ_rOEWAynx",
        "outputId": "c0ec78f7-9352-4cbd-9bca-0c5f22c01b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------+\n",
            "| id|valor_exploded|\n",
            "+---+--------------+\n",
            "|  A|             1|\n",
            "|  A|             2|\n",
            "|  A|             3|\n",
            "|  B|             4|\n",
            "|  B|             5|\n",
            "|  C|             6|\n",
            "+---+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df_exploded = df.select('id', explode('valores').alias('valor_exploded'))\n",
        "df_exploded.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKfMbkirDS1M"
      },
      "source": [
        "### Funciones con String\n",
        "\n",
        "| Método | Descripción |\n",
        "| --- | --- |\n",
        "| translate | Translate any character in the src by a character in replaceString |\n",
        "| regexp_replace | Replace all substrings of the specified string value that match regexp with rep |\n",
        "| regexp_extract | Extract a specific group matched by a Java regex, from the specified string column |\n",
        "| ltrim | Removes the leading space characters from the specified string column |\n",
        "| lower | Converts a string column to lowercase |\n",
        "| split | Splits str around matches of the given pattern |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icxUss11DKgh",
        "outputId": "8a7a125d-0dbc-4370-9e22-4a6cd2256867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|           col|\n",
            "+--------------+\n",
            "|       correo1|\n",
            "|     gmail.com|\n",
            "|       correo2|\n",
            "|   hotmail.com|\n",
            "|       correo3|\n",
            "|murciaeduca.es|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import split\n",
        "\n",
        "data_to_split = [(1, \"correo1@gmail.com\"),\n",
        "        (2, 'correo2@hotmail.com'),\n",
        "        (3, 'correo3@murciaeduca.es')]\n",
        "\n",
        "df_mail = spark.createDataFrame(data_to_split, schema='id int, email string')\n",
        "df_email_handle = df_mail.select(split(df_mail.email, '@', 0).alias('email_handle'))\n",
        "\n",
        "df_email_handle.select(explode('email_handle')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vh3eUeXG-90"
      },
      "source": [
        "### Functions de colecciones\n",
        "\n",
        "| Método | Descripción |\n",
        "| --- | --- |\n",
        "| array_contains | Returns null if the array is null, true if the array contains value, and false otherwise. |\n",
        "| element_at | Returns element of array at given index. Array elements are numbered starting with **1**. |\n",
        "| explode | Creates a new row for each element in the given array or map column. |\n",
        "| collect_set | Returns a set of objects with duplicate elements eliminated. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OBzggnGFTpZ",
        "outputId": "9652cf33-a569-436e-d864-eabbf06266bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| _1|                  _2|\n",
            "+---+--------------------+\n",
            "|  1|[correo1@gmail.co...|\n",
            "|  2|[correo2@hotmail....|\n",
            "|  3|[correo3@murciaed...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_to_filter = [(1, ['correo1@gmail.com', 'correo2@gmail.com']),\n",
        "        (2, ['correo2@hotmail.com', 'correo3@gmail.com']),\n",
        "        (3, ['correo3@murciaeduca.es'])]\n",
        "\n",
        "df = spark.createDataFrame(data_to_filter)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdFLeAjlHwqQ",
        "outputId": "b7b70d60-700e-4134-dd01-64faa23b82e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| _1|                  _2|\n",
            "+---+--------------------+\n",
            "|  1|[correo1@gmail.co...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_df = (\n",
        "    df.filter(array_contains(col('_2'), 'correo1@gmail.com'))\n",
        ")\n",
        "\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAFgJKXsJYjK"
      },
      "source": [
        "## Union y unionByName\n",
        "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Advertencia\"> El método <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html\" target=\"_blank\">**`union`**</a> de DataFrame resuelve las columnas por posición, como en SQL estándar. Deberías usarlo solo si los dos DataFrames tienen exactamente el mismo esquema, incluido el orden de las columnas. En cambio, el método <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html\" target=\"_blank\">**`unionByName`**</a> de DataFrame resuelve las columnas por nombre. Esto es equivalente a UNION ALL en SQL. Ninguno de los dos eliminará duplicados.\n",
        "\n",
        "A continuación se muestra una verificación para ver si los dos DataFrames tienen un esquema coincidente donde sería apropiado usar **`union`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuXb1-vgJhlN",
        "outputId": "f8f14822-4b1f-4093-b2dc-fead6fdc9674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.schema == filtered_df.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkkIVYD2J0AA",
        "outputId": "00c9a003-d9c8-4c96-93d9-7dd6f051821f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| _1|                  _2|\n",
            "+---+--------------------+\n",
            "|  1|[correo1@gmail.co...|\n",
            "|  2|[correo2@hotmail....|\n",
            "|  3|[correo3@murciaed...|\n",
            "|  1|[correo1@gmail.co...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_union = df.union(filtered_df)\n",
        "df_union.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ZnYmyNKVIO"
      },
      "source": [
        "### Funciones miscelánea\n",
        "\n",
        "| Método | Descripción |\n",
        "| --- | --- |\n",
        "| col / column | Returns a Column based on the given column name. |\n",
        "| lit | Creates a Column of literal value |\n",
        "| isnull | Return true if the column is null |\n",
        "| rand | Generate a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYMEHE1KEcZ",
        "outputId": "e2f32b08-9d35-47a2-bdfd-7c03b280e92f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------------+\n",
            "| id|            email|\n",
            "+---+-----------------+\n",
            "|  1|correo1@gmail.com|\n",
            "+---+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gmail_accounts = df_mail.filter(col('email').endswith('gmail.com'))\n",
        "gmail_accounts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7O0tZ8lKujt",
        "outputId": "0f82113c-72cd-4790-ba11-1ed8e52475b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+----------+\n",
            "|            email|gmail user|\n",
            "+-----------------+----------+\n",
            "|correo1@gmail.com|      true|\n",
            "+-----------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_gmail_user = gmail_accounts.select('email', lit(True).alias('gmail user'))\n",
        "df_gmail_user.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i54YdJkFM3Z2"
      },
      "source": [
        "### DataFrameNaFunctions\n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.html#pyspark.sql.DataFrameNaFunctions\" target=\"_blank\">DataFrameNaFunctions</a> es un submódulo de DataFrame con métodos para manejar valores nulos. Obtén una instancia de DataFrameNaFunctions accediendo al atributo na de un DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzIipRwbLikR",
        "outputId": "67288fe6-d939-4235-e03e-089c97964341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  NULL|NULL| NULL|              [|\n",
            "|     I|10.0| 8.04|           NULL|\n",
            "|     I| 8.0| 6.95|           NULL|\n",
            "|     I|13.0| 7.58|           NULL|\n",
            "|     I| 9.0| 8.81|           NULL|\n",
            "|     I|11.0| 8.33|           NULL|\n",
            "|     I|14.0| 9.96|           NULL|\n",
            "|     I| 6.0| 7.24|           NULL|\n",
            "|     I| 4.0| 4.26|           NULL|\n",
            "|     I|12.0|10.84|           NULL|\n",
            "|     I| 7.0| 4.81|           NULL|\n",
            "|     I| 5.0| 5.68|           NULL|\n",
            "|    II|10.0| 9.14|           NULL|\n",
            "|    II| 8.0| 8.14|           NULL|\n",
            "|    II|13.0| 8.74|           NULL|\n",
            "|    II| 9.0| 8.77|           NULL|\n",
            "|    II|11.0| 9.26|           NULL|\n",
            "|    II|14.0|  8.1|           NULL|\n",
            "|    II| 6.0| 6.13|           NULL|\n",
            "|    II| 4.0|  3.1|           NULL|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = (spark\n",
        "    .read\n",
        "    .option('inferSchema', True)\n",
        "    .json('/content/sample_data/anscombe.json')\n",
        "  )\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlfmJ88IMAqP",
        "outputId": "bd668209-dc58-426e-911f-99a0fbd0ca86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(df.count())\n",
        "print(df.na.drop().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkIzbS-2Mgew",
        "outputId": "ab9c3b6e-712f-4d64-ae91-b876c49ffb17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select('Series').na.drop().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0j__8yfMqvc",
        "outputId": "060bff1a-8d7a-4418-dd95-54be85578178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|   Series|\n",
            "+---------+\n",
            "|NO SERIES|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|        I|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "|       II|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('Series').na.fill('NO SERIES').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-RmeO1NOoX"
      },
      "source": [
        "### Unión de DataFrames\n",
        "El método <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join\" target=\"_blank\">join</a> de DataFrame une dos DataFrames basándose en una expresión de unión dada.\n",
        "\n",
        "Se admiten varios tipos diferentes de uniones:\n",
        "\n",
        "Unión interna basada en valores iguales de una columna compartida llamada \"name\" (es decir, una unión equitativa)<br/>\n",
        "`df1.join(df2, \"name\")`\n",
        "\n",
        "Unión interna basada en valores iguales de las columnas compartidas llamadas \"name\" y \"age\"<br/>\n",
        "`df1.join(df2, [\"name\", \"age\"])`\n",
        "\n",
        "Unión externa completa basada en valores iguales de una columna compartida llamada \"name\"<br/>\n",
        "`df1.join(df2, \"name\", \"outer\")`\n",
        "\n",
        "Unión externa izquierda basada en una expresión explícita de columna<br/>\n",
        "`df1.join(df2, df1[\"customer_name\"] == df2[\"account_name\"], \"left_outer\")`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JlZZoTHNEQB",
        "outputId": "b453b755-b922-43cf-cc37-6d4769a77a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+---+----------+\n",
            "|            email| id|gmail user|\n",
            "+-----------------+---+----------+\n",
            "|correo1@gmail.com|  1|      true|\n",
            "+-----------------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df = gmail_accounts.join(other=df_gmail_user, on='email', how = \"inner\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
